{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('tf2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "865b91303c0655b63e42c5d03cbd424f89ab1078fc8eeb577e3059958e267924"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 包导入\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "# 验证小区上的流量，标签为小区下一个小时流量\n",
    "# y_true为小区下个小时的流量\n",
    "# 数据集为[24,4]的张量，将([i,3]+[i,4])=y_train[]\n",
    "# 数据为：平均人数，最大人数，上行流量，下行流量，总流量（标签）\n",
    "# 用于训练时总流量要向后移动一小时\n",
    "# #\n",
    "\n",
    "\n",
    "# 数据生成的参数设置\n",
    "# #\n",
    "hour_rate = [0.1, 0.2, 0.3, 0.1, 0.2, 0.3, 0.1, 0.2, 0.3, 0.1, 0.2, 0.3, \n",
    "             0.1, 0.2, 0.3, 0.1, 0.2, 0.3, 0.1, 0.2, 0.3, 0.1,0.2, 0.3]\n",
    "people = 1000\n",
    "down_flow = 5000.0\n",
    "up_flow = 100.0\n",
    "file_path = \"data.tfrecord\"\n",
    "\n",
    "\n",
    "# 写数据到TFrecord文件\n",
    "# #\n",
    "# 生成首个数据，接下来的数据在前23小时上加一小时\n",
    "first_day = np.zeros(120)\n",
    "all_day = np.zeros(120000)\n",
    "for i in range(0, 24):\n",
    "    avg_people = hour_rate[i%24] * random.randint(30,80) * people / 10 \n",
    "    max_people = avg_people * random.randint(20, 40) / 10\n",
    "    down_f = hour_rate[i%24] * random.randint(30,80) * down_flow / 10\n",
    "    up_f = hour_rate[i%24] * random.randint(30, 80) *  up_flow / 10\n",
    "    first_day[i*5] = avg_people\n",
    "    first_day[i*5+1] = max_people\n",
    "    first_day[i*5+2] = up_f\n",
    "    first_day[i*5+3] = down_f\n",
    "    first_day[i*5+4] = up_f + down_f\n",
    "\n",
    "# 储存第一组数据到总数据数组\n",
    "for i in range(0, 120):\n",
    "    all_day[i] = first_day[i] \n",
    "\n",
    "pre_day = first_day\n",
    "for j in range(1, 1000):\n",
    "    next_day = np.zeros(120)\n",
    "    for m in range(0, 115):\n",
    "        next_day[m] = pre_day[m+5]\n",
    "        #输入新加的5个数据\n",
    "        avg_people = hour_rate[j%24] * random.randint(30,80) * people / 10 \n",
    "        max_people = avg_people * random.randint(20, 40) / 10\n",
    "        down_f = hour_rate[j%24] * random.randint(30,80) * down_flow / 10\n",
    "        up_f = hour_rate[j%24] * random.randint(30, 80) *  up_flow / 10\n",
    "        next_day[115] = avg_people\n",
    "        next_day[116] = max_people\n",
    "        next_day[117] = up_f\n",
    "        next_day[118] = down_f\n",
    "        next_day[119] = up_f + down_f\n",
    "    for n in range(0, 120):\n",
    "        all_day[j*120+n] = next_day[n]\n",
    "    pre_day = next_day\n",
    "# 写入数据\n",
    "with tf.io.TFRecordWriter(file_path) as files:\n",
    "    # 第一个数据的写入\n",
    "    example_first = tf.train.Example(\n",
    "        features=tf.train.Features(\n",
    "            feature={\n",
    "                \"data\": tf.train.Feature(\n",
    "                    float_list=tf.train.FloatList(value=all_day))\n",
    "                }))\n",
    "    files.write(example_first.SerializePartialToString())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读数据函数,返回解析后的结果\n",
    "def read_datas(filepath):\n",
    "    feature_description = {\n",
    "        \"data\": tf.io.VarLenFeature(dtype=float)\n",
    "    }\n",
    "\n",
    "    file_dataset = tf.data.TFRecordDataset(filepath).batch(5)\n",
    "\n",
    "    for serialized_example in file_dataset:\n",
    "        parsed_example = tf.io.parse_example(serialized_example,                                                                                                       feature_description)\n",
    "    \n",
    "    return parsed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据并简单处理\n",
    "datas = read_datas(file_path)[\"data\"].values\n",
    "reshape_datas = tf.reshape(datas, [24000,5])\n",
    "x_data = reshape_datas[0:24000,0:4]\n",
    "y_data = reshape_datas[0:24000,4]\n",
    "print(reshape_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(\n[[[[ 670.]\n   [2345.]\n   [  47.]\n   [3900.]]\n\n  [[1080.]\n   [4320.]\n   [ 128.]\n   [5800.]]\n\n  [[1440.]\n   [5040.]\n   [ 117.]\n   [9000.]]\n\n  [[ 780.]\n   [2028.]\n   [  41.]\n   [3100.]]\n\n  [[ 860.]\n   [2752.]\n   [ 138.]\n   [4300.]]\n\n  [[1800.]\n   [4500.]\n   [ 114.]\n   [7800.]]\n\n  [[ 640.]\n   [1664.]\n   [  45.]\n   [3600.]]\n\n  [[ 840.]\n   [1764.]\n   [ 108.]\n   [8000.]]\n\n  [[1290.]\n   [3096.]\n   [ 177.]\n   [8250.]]\n\n  [[ 670.]\n   [1407.]\n   [  40.]\n   [2350.]]\n\n  [[ 720.]\n   [2376.]\n   [ 116.]\n   [6400.]]\n\n  [[1950.]\n   [5850.]\n   [ 114.]\n   [8250.]]\n\n  [[ 500.]\n   [1950.]\n   [  33.]\n   [1650.]]\n\n  [[1100.]\n   [2640.]\n   [  96.]\n   [3200.]]\n\n  [[1500.]\n   [4500.]\n   [ 138.]\n   [5700.]]\n\n  [[ 490.]\n   [1274.]\n   [  57.]\n   [2000.]]\n\n  [[ 740.]\n   [2960.]\n   [  94.]\n   [5100.]]\n\n  [[1890.]\n   [5292.]\n   [ 105.]\n   [7050.]]\n\n  [[ 370.]\n   [ 814.]\n   [  53.]\n   [1850.]]\n\n  [[1420.]\n   [2982.]\n   [ 102.]\n   [6800.]]\n\n  [[ 930.]\n   [2790.]\n   [ 144.]\n   [9900.]]\n\n  [[ 500.]\n   [1850.]\n   [  76.]\n   [3300.]]\n\n  [[1160.]\n   [3248.]\n   [  62.]\n   [6300.]]\n\n  [[1410.]\n   [5076.]\n   [ 165.]\n   [9750.]]]], shape=(1, 24, 4, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "_data = np.array(x_data)\n",
    "data_4 = np.zeros(shape=[1,24,4,1])\n",
    "for i in range(0, 24):\n",
    "    for j in range(0, 4):\n",
    "          data_4[0,i,j,0] = _data[i,j]\n",
    "\n",
    "train_4 = tf.constant(data_4, dtype=float)\n",
    "print(train_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data数据展示\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_data数据展示\n",
    "print(y_data[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_all是每个数据张量的下一个数据张量的第一行第五列\n",
    "y_all_data = np.zeros(1000)\n",
    "for i in range(0, 1000):\n",
    "    y_all_data[i] = y_data[i*24+1]\n",
    "y_all = tf.constant(y_all_data, dtype=float)\n",
    "print(y_all) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练数据，验证数据，测试数据的构建\n",
    "# 比例为0.7：0.2：0.1\n",
    "\n",
    "y_train = y_all[0:700]\n",
    "X_train = x_data[0:16800,0:4]\n",
    "\n",
    "y_evaluate = y_all[700:900]\n",
    "X_evaluate = x_data[16800:21600,0:4]\n",
    "\n",
    "y_test = y_all[900:1000]\n",
    "X_test = x_data[21600:24000,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################模型构建####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN模块创建\n",
    "# 步频为1，输入输出的空间维度不变[24,4]\n",
    "udn_model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(128, 2, activation=\"relu\", padding=\"same\",\n",
    "                         input_shape=[24, 4, 1]),\n",
    "    keras.layers.Conv2D(64, 2, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.Conv2D(32, 2, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.Conv2D(32, 2, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.MaxPool2D(2),\n",
    "    keras.layers.Conv2D(16, 2, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1, activation=\"relu\")\n",
    "])\n",
    "\n",
    "udn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(8364.0, shape=(), dtype=float32)\ntf.Tensor([1.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y = y_all[10]\n",
    "y_4 = tf.constant([1.0])\n",
    "print(y)\n",
    "print(y_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 1 samples\n",
      "1/1 [==============================] - 1s 1s/sample - loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(128, 2, activation=\"relu\", padding=\"same\",\n",
    "                        input_shape=[24,4,1])\n",
    "])\n",
    "\n",
    "SGD = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\n",
    "model.compile(\n",
    "    loss=negative_log_likelihood,\n",
    "    optimizer=SGD\n",
    ")\n",
    "\n",
    "y = model.fit(\n",
    "    x=train_4,\n",
    "    y=y_4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数为负对数似然（negative log likelihood）\n",
    "def negative_log_likelihood(y_true, y_pred, eps=1e-15):\n",
    "    p = keras.backend.clip(y_pred, eps, 1-eps)\n",
    "    loss = keras.backend.sum(-y_true * keras.backend.log(p) - (1-y_true) * keras.backend.log(1-p))\n",
    "    return keras.backend.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编译模块\n",
    "SGD = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\n",
    "udn_model.compile(\n",
    "    loss=negative_log_likelihood,\n",
    "    optimizer=SGD\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (4, 28, 28, 3)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.keras.layers.Conv2D(128, 2, activation='relu', padding=\"same\",input_shape=[24, 4, 1])(train_4)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模块\n",
    "# 在训练时运用mini-batch方法进行小批量梯度、\n",
    "# 对每个小时进行预测\n",
    "udn_model.fit(\n",
    "    x=train_4,\n",
    "    y=y_all[0],\n",
    "    callbacks=\"EarlyStopping\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证模块\n",
    "udn_model.evaluate(\n",
    "\n",
    ")"
   ]
  }
 ]
}